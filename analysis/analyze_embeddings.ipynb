{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import ruamel.yaml as yaml\n",
    "import umap\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from src.models.model_diff_modular import ModularDiffModel\n",
    "from src.adv_attack import get_hidden_dataloader\n",
    "from src.utils import get_num_labels, dict_to_device\n",
    "from src.data_handler import get_data_loader_bios, read_label_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sorted(args_train):\n",
    "    num_labels = get_num_labels(args_train.labels_task_path)\n",
    "    num_labels_protected = get_num_labels(args_train.labels_protected_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args_train.model_name)\n",
    "    train_loader = get_data_loader_bios(\n",
    "        tokenizer = tokenizer,\n",
    "        data_path = args_train.train_pkl,\n",
    "        labels_task_path = args_train.labels_task_path,\n",
    "        labels_prot_path = args_train.labels_protected_path,\n",
    "        batch_size = args_train.batch_size,\n",
    "        max_length = 200,\n",
    "        shuffle = False,\n",
    "        debug = False\n",
    "    )\n",
    "    val_loader = get_data_loader_bios(\n",
    "        tokenizer = tokenizer,\n",
    "        data_path = args_train.val_pkl,\n",
    "        labels_task_path = args_train.labels_task_path,\n",
    "        labels_prot_path = args_train.labels_protected_path,\n",
    "        batch_size = args_train.batch_size,\n",
    "        max_length = 200,\n",
    "        shuffle = False,\n",
    "        debug = False\n",
    "    )\n",
    "    with open(args_train.train_pkl, 'rb') as file:\n",
    "        data_dicts_train = pickle.load(file)\n",
    "    with open(args_train.val_pkl, 'rb') as file:\n",
    "        data_dicts_val = pickle.load(file)\n",
    "    return train_loader, val_loader, num_labels, num_labels_protected, data_dicts_train, data_dicts_val\n",
    "\n",
    "def get_embeddings(model, loader):\n",
    "        model.eval()\n",
    "        emb_list = []\n",
    "        label_list_task = []\n",
    "        label_list_prot = []\n",
    "        for batch in tqdm(loader, desc=\"generating embeddings\"):\n",
    "            inputs, labels_task, labels_prot = batch\n",
    "            inputs = dict_to_device(inputs, model.device)\n",
    "            hidden = model._forward(**inputs)\n",
    "            emb_list.append(hidden.cpu())\n",
    "            label_list_task.append(labels_task)\n",
    "            label_list_prot.append(labels_prot)\n",
    "        return torch.cat(emb_list), torch.cat(label_list_task), torch.cat(label_list_prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../cfg.yml\", \"r\") as f:\n",
    "#     cfg = yaml.safe_load(f)\n",
    "# args_train = argparse.Namespace(**cfg[\"train_config\"], **cfg[\"data_config_bios\"], **cfg[\"model_config\"])\n",
    "\n",
    "# setattr(args_train, \"train_pkl\", \"/share/cp/datasets/nlp/text_classification_bias/bios/only_task_balanced/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, val_loader, num_labels, num_labels_protected, data_dicts_train, data_dicts_val = get_data_sorted(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cp_dir = \"/share/home/lukash/checkpoints_bert_L4/seed0\"\n",
    "# # cp = \"bert_uncased_L-4_H-256_A-4-fixmask0.1-modular.pt\"\n",
    "# cp_dir = \"/share/home/lukash/checkpoints_bert_L4/seed{}\"\n",
    "# cp = \"bert_uncased_L-4_H-256_A-4-fixmask0.1-modular-sparse_task.pt\"\n",
    "# cp_dir = \"../checkpoints_bios\"\n",
    "# cp = \"bert_uncased_L-4_H-256_A-4-modular_baseline-seed{}.pt\"\n",
    "\n",
    "# data = []\n",
    "# for i in range(4):\n",
    "#     filepath = os.path.join(cp_dir.format(i), cp)\n",
    "#     model_biased = ModularDiffModel.load_checkpoint(filepath, remove_parametrizations=True, debiased=False)\n",
    "#     model_debiased = ModularDiffModel.load_checkpoint(filepath, remove_parametrizations=True, debiased=True)\n",
    "#     setattr(args_train, \"model_name\", model_biased.model_name)\n",
    "\n",
    "#     model_biased.to(\"cuda:2\")\n",
    "#     model_debiased.to(\"cuda:2\")\n",
    "\n",
    "#     emb_train_biased, labels_train_task, labels_train_prot = get_embeddings(model_biased, train_loader)\n",
    "#     emb_val_biased, labels_val_task, labels_val_prot = get_embeddings(model_biased, val_loader)\n",
    "#     emb_train_debiased, _, _ = get_embeddings(model_debiased, train_loader)\n",
    "#     emb_val_debiased, _, _ = get_embeddings(model_debiased, val_loader)\n",
    "#     data.append([emb_train_biased, emb_train_debiased, emb_val_biased, emb_val_debiased, labels_train_task, labels_val_task, labels_train_prot, labels_val_prot])\n",
    "\n",
    "#     model_biased.cpu()\n",
    "#     model_debiased.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(output_folder, \"modular_diff_embeddings.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_folder, \"modular_diff_embeddings.pkl\"), \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train_biased, emb_train_debiased, emb_val_biased, emb_val_debiased, labels_train_task, labels_val_task, labels_train_prot, labels_val_prot = [torch.cat(x) for x in list(zip(*data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "cp_idx_train = torch.kron(torch.arange(i), torch.full((emb_train_biased.shape[0] // i,), 1))\n",
    "cp_idx_val = torch.kron(torch.arange(i), torch.full((emb_val_biased.shape[0] // i,), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = torch.cat([emb_train_biased, emb_train_debiased])\n",
    "val_embeddings = torch.cat([emb_val_biased, emb_val_debiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap_reducer = umap.UMAP()\n",
    "# emb_umap = umap_reducer.fit_transform(torch.cat([train_embeddings, val_embeddings]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(output_folder, \"modular_diff_embeddings_umap.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump((emb_umap, umap_reducer), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_folder, \"modular_diff_embeddings_umap.pkl\"), \"rb\") as f:\n",
    "    emb_umap, umap_reducer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_reducer = TSNE()\n",
    "# emb_tsne = tsne_reducer.fit_transform(torch.cat([train_embeddings, val_embeddings]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_umap_train = emb_umap[:train_embeddings.shape[0]]\n",
    "emb_umap_val = emb_umap[train_embeddings.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_train = emb_train_biased.shape[0]\n",
    "cutoff_val = emb_val_biased.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dict = read_label_file(\"/share/cp/datasets/nlp/text_classification_bias/bios/labels_task.txt\")\n",
    "gender_dict = read_label_file(\"/share/cp/datasets/nlp/text_classification_bias/bios/labels_protected_gender.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = np.concatenate([\n",
    "#     np.zeros((emb_dict[\"biased_train\"][0].shape[0],), int),\n",
    "#     np.ones((emb_dict[\"debiased_train\"][0].shape[0],), int)\n",
    "# ], axis=0)\n",
    "# labels = [\"biased\"] * emb_dict[\"biased_train\"][0].shape[0] + [\"debiased\"] * emb_dict[\"debiased_train\"][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10,10))\n",
    "axs[0,0].scatter(x=emb_umap_train[:cutoff_train,0], y=emb_umap_train[:cutoff_train,1], c=\"blue\", edgecolors='black')\n",
    "axs[0,0].set_title(\"Training Data - Biased\")\n",
    "axs[0,1].scatter(x=emb_umap_train[cutoff_train:,0], y=emb_umap_train[cutoff_train:,1], c=\"orange\", edgecolors='black')\n",
    "axs[0,1].set_title(\"Training Data - Debiased\")\n",
    "axs[1,0].scatter(x=emb_umap_val[:cutoff_val,0], y=emb_umap_val[:cutoff_val,1], c=\"blue\", edgecolors='black')\n",
    "axs[1,0].set_title(\"Valdiation Data - Biased\")\n",
    "axs[1,1].scatter(x=emb_umap_val[cutoff_val:,0], y=emb_umap_val[cutoff_val:,1], c=\"orange\", edgecolors='black')\n",
    "axs[1,1].set_title(\"Validation Data - Debiased\")\n",
    "fig.suptitle('UMAP Embeddings')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart_dict = {}\n",
    "# for job_title, job_idx in tqdm(job_dict.items()):\n",
    "\n",
    "#     f_train = (labels_train_task == job_idx)\n",
    "#     f_val = (labels_val_task == job_idx)\n",
    "#     _train_embeddings = torch.cat([emb_train_biased[f_train], emb_train_debiased[f_train]])\n",
    "#     _val_embeddings = torch.cat([emb_val_biased[f_val], emb_val_debiased[f_val]])\n",
    "\n",
    "#     _umap_reducer = umap.UMAP()\n",
    "#     _emb_umap = _umap_reducer.fit_transform(torch.cat([_train_embeddings, _val_embeddings]).numpy())\n",
    "#     _emb_umap_train = _emb_umap[:_train_embeddings.shape[0]]\n",
    "#     _emb_umap_val = _emb_umap[_train_embeddings.shape[0]:]\n",
    "\n",
    "#     _c_train = [\"blue\" if x else \"orange\" for x in labels_train_prot[f_train]]\n",
    "#     _c_val = [\"blue\" if x else \"orange\" for x in labels_val_prot[f_val]]\n",
    "\n",
    "#     _cutoff_train = f_train.sum().item()\n",
    "#     _cutoff_val = f_val.sum().item()\n",
    "\n",
    "#     _emb_umap_train_biased = _emb_umap_train[:_cutoff_train]\n",
    "#     _emb_umap_train_debiased = _emb_umap_train[_cutoff_train:]\n",
    "#     _emb_umap_val_biased = _emb_umap_val[:_cutoff_val]\n",
    "#     _emb_umap_val_debiased = _emb_umap_val[_cutoff_val:]\n",
    "\n",
    "#     chart_dict[job_idx] = (_emb_umap_train_biased, _emb_umap_train_debiased, _c_train, _emb_umap_val_biased, _emb_umap_val_debiased, _c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"job_emb_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(chart_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_folder, \"job_emb_dict.pkl\"), \"rb\") as f:\n",
    "    chart_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_idx, data in chart_dict.items():\n",
    "    job_title = {v:k for k,v in job_dict.items()}[job_idx]\n",
    "\n",
    "    _emb_umap_train_biased, _emb_umap_train_debiased, _c_train, _emb_umap_val_biased, _emb_umap_val_debiased, _c_val = data\n",
    "\n",
    "    l = [Line2D([0], [0], marker='o', color=c) for c in [\"orange\", \"blue\"]]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10,10))\n",
    "    axs[0,0].scatter(x=_emb_umap_train_biased[:,0], y=_emb_umap_train_biased[:,1], c=_c_train, edgecolors='black')\n",
    "    axs[0,0].set_title(\"Training Data - Biased\")\n",
    "    axs[0,1].scatter(x=_emb_umap_train_debiased[:,0], y=_emb_umap_train_debiased[:,1], c=_c_train, edgecolors='black')\n",
    "    axs[0,1].set_title(\"Training Data - Debiased\")\n",
    "    axs[1,0].scatter(x=_emb_umap_val_biased[:,0], y=_emb_umap_val_biased[:,1], c=_c_val, edgecolors='black')\n",
    "    axs[1,0].set_title(\"Valdiation Data - Biased\")\n",
    "    axs[1,1].scatter(x=_emb_umap_val_debiased[:,0], y=_emb_umap_val_debiased[:,1], c=_c_val, edgecolors='black')\n",
    "    axs[1,1].set_title(\"Validation Data - Debiased\")\n",
    "    fig.legend(l, [\"Male\", \"Female\"])\n",
    "    fig.suptitle(f'UMAP Embeddings - {\" \".join([x.capitalize() for x in job_title.split(\"_\")])}')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f\"umap_{job_title}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for job_title, job_idx in job_dict.items():\n",
    "\n",
    "#     emb_train_biased = emb_umap_train[:cutoff_train][(labels_train_task == job_idx)]\n",
    "#     emb_train_debiased = emb_umap_train[cutoff_train:][(labels_train_task == job_idx)]\n",
    "#     emb_val_biased = emb_umap_val[:cutoff_val][(labels_val_task == job_idx)]\n",
    "#     emb_val_debiased = emb_umap_val[cutoff_val:][(labels_val_task == job_idx)]\n",
    "#     c_train = [\"blue\" if x else \"orange\" for x in labels_train_prot[(labels_train_task == job_idx)]]\n",
    "#     c_val = [\"blue\" if x else \"orange\" for x in labels_val_prot[(labels_val_task == job_idx)]]\n",
    "\n",
    "#     fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10,10))\n",
    "#     axs[0,0].scatter(x=emb_train_biased[:,0], y=emb_train_biased[:,1], c=c_train, edgecolors='black')\n",
    "#     axs[0,0].set_title(\"Training Data - Biased\")\n",
    "#     axs[0,1].scatter(x=emb_train_debiased[:,0], y=emb_train_debiased[:,1], c=c_train, edgecolors='black')\n",
    "#     axs[0,1].set_title(\"Training Data - Debiased\")\n",
    "#     axs[1,0].scatter(x=emb_val_biased[:,0], y=emb_val_biased[:,1], c=c_val, edgecolors='black')\n",
    "#     axs[1,0].set_title(\"Valdiation Data - Biased\")\n",
    "#     axs[1,1].scatter(x=emb_val_debiased[:,0], y=emb_val_debiased[:,1], c=c_val, edgecolors='black')\n",
    "#     axs[1,1].set_title(\"Validation Data - Debiased\")\n",
    "#     fig.suptitle(f'UMAP Embeddings - {job_title}')\n",
    "#     fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = torch.norm(emb_train_biased - emb_train_debiased, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, idx = torch.topk(distances, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [data_dicts_train[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(d[\"title\"], d[\"gender\"], d[\"bio\"]) for d in dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = emb_train_biased.shape[1]\n",
    "linear_transform = torch.nn.Linear(sh,sh)\n",
    "loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "opt = torch.optim.SGD(linear_transform.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(emb_train_biased, emb_train_debiased)\n",
    "loader = DataLoader(ds, shuffle=True, batch_size=32, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform.train()\n",
    "s = \"loss: {:.2f}\"\n",
    "losses = []\n",
    "train_iterator = trange(20, desc=s.format(math.nan), position=0, leave=False)\n",
    "for epoch in train_iterator:\n",
    "    l_list = []\n",
    "    for x,y in tqdm(loader, position=1, leave=False):\n",
    "        y_hat = linear_transform(x)\n",
    "        l = loss(y_hat, y)\n",
    "        l.mean().backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        l_list.append(l.sum(1).detach())\n",
    "    avg_loss = torch.cat(l_list).mean()\n",
    "    losses.append(avg_loss)\n",
    "    train_iterator.set_description(s.format(avg_loss), refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('testenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44fd53a9831942cc7290b70e18d76362301b009d310fe9edb8a5f7b8f5560d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
